{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9796d901",
   "metadata": {},
   "source": [
    "# WEB SCRAPING ‚Äì ASSIGNMENT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3712d8a",
   "metadata": {},
   "source": [
    "Q1: In this question you have to scrape data using the filters available on the webpage You have to use the location and salary filter.\n",
    "You have to scrape data for ‚ÄúData Scientist‚Äù designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is ‚ÄúDelhi/NCR‚Äù. The salary filter to be used is ‚Äú3-6‚Äù lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the web page https://www.naukri.com/\n",
    "2. Enter ‚ÄúData Scientist‚Äù in ‚ÄúSkill, Designations, and Companies‚Äù field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4423b06",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cbb3637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70924123",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66852c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the page which is given web page naukari.com\n",
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc06b713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entering designations column in Data Scientist\n",
    "\n",
    "designations=driver.find_element(By.CLASS_NAME,\"suggestor-input \")\n",
    "designations.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a4cf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0a76b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply location filter - Delhi/NCR\n",
    "\n",
    "location_filter = driver.find_element(By.XPATH,\"/html/body/div/div/main/div[1]/div[1]/div/div/div[2]/div[5]/div[2]/div[2]/label/i\")\n",
    "location_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3a92949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Salary Filter - 3-6 lakh\n",
    "\n",
    "salary_filter=driver.find_element(By.XPATH,\"/html/body/div/div/main/div[1]/div[1]/div/div/div[2]/div[6]/div[2]/div[2]/label/i\")\n",
    "salary_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19ec8ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store data\n",
    "job_titles = []\n",
    "job_locations = []\n",
    "company_names = []\n",
    "experience_required = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73c6665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape data for the first 10 job results\n",
    "job_results = driver.find_elements(By.CLASS_NAME, \"jobTuple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34c14736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping job title from the given page\n",
    "titles_tags=driver.find_elements(By.XPATH,'//div[@class=\"cust-job-tuple layout-wrapper lay-2 sjw__tuple \"]/div/a')\n",
    "for i in titles_tags:\n",
    "    titles=i.text\n",
    "    job_titles.append(titles)\n",
    "    \n",
    "# Scraping job location from the given page\n",
    "locations_tags=driver.find_elements(By.XPATH,'//span[@class=\"locWdth\"]')\n",
    "for i in locations_tags:\n",
    "    locations=i.text\n",
    "    job_locations.append(locations)\n",
    "    \n",
    "# Scraping company name from the given page\n",
    "company_tags=driver.find_elements(By.XPATH,'//div[@class=\" row2\"]/span/a[1]')\n",
    "for i in company_tags:\n",
    "    company=i.text\n",
    "    company_names.append(company)\n",
    "    \n",
    "# Scraping job Exprience form the given page\n",
    "experience_tags=driver.find_elements(By.XPATH,'//span[@class=\"expwdth\"]')\n",
    "for i in experience_tags:\n",
    "    experience=i.text\n",
    "    experience_required.append(experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2660ee74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20 20 20\n"
     ]
    }
   ],
   "source": [
    "print(len(job_titles),len(job_locations),len(company_names),len(experience_required))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "909ba8ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist HTHD</td>\n",
       "      <td>Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...</td>\n",
       "      <td>Ford</td>\n",
       "      <td>1-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurugram, Bengaluru</td>\n",
       "      <td>Blackbuck</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Associate Scientist - Data Sourcing &amp; Solutions</td>\n",
       "      <td>Gurugram</td>\n",
       "      <td>Axa Technology Services</td>\n",
       "      <td>1-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Global GI Data Scientist</td>\n",
       "      <td>Faridabad</td>\n",
       "      <td>Hitachi Ltd.</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Global GI Data Scientist</td>\n",
       "      <td>Faridabad</td>\n",
       "      <td>Hitachi Energy</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...</td>\n",
       "      <td>Digital Glyde</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Python Data Scientist</td>\n",
       "      <td>Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...</td>\n",
       "      <td>Wizaltia Hr Solutions</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...</td>\n",
       "      <td>Kmart</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...</td>\n",
       "      <td>KAS Services</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...</td>\n",
       "      <td>Scienaptic Systems</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Title  \\\n",
       "0                              Data Scientist HTHD   \n",
       "1                                   Data Scientist   \n",
       "2  Associate Scientist - Data Sourcing & Solutions   \n",
       "3                         Global GI Data Scientist   \n",
       "4                         Global GI Data Scientist   \n",
       "5                                   Data Scientist   \n",
       "6                            Python Data Scientist   \n",
       "7                                   Data Scientist   \n",
       "8                                   Data Scientist   \n",
       "9                                   Data Scientist   \n",
       "\n",
       "                                            Location                  Company  \\\n",
       "0  Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...                     Ford   \n",
       "1                                Gurugram, Bengaluru                Blackbuck   \n",
       "2                                           Gurugram  Axa Technology Services   \n",
       "3                                          Faridabad             Hitachi Ltd.   \n",
       "4                                          Faridabad           Hitachi Energy   \n",
       "5  Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...            Digital Glyde   \n",
       "6  Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...    Wizaltia Hr Solutions   \n",
       "7  Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...                    Kmart   \n",
       "8  Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...             KAS Services   \n",
       "9  Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...       Scienaptic Systems   \n",
       "\n",
       "  Experience  \n",
       "0    1-4 Yrs  \n",
       "1    3-7 Yrs  \n",
       "2    1-4 Yrs  \n",
       "3    3-5 Yrs  \n",
       "4    3-5 Yrs  \n",
       "5    3-7 Yrs  \n",
       "6    2-7 Yrs  \n",
       "7    3-5 Yrs  \n",
       "8    3-5 Yrs  \n",
       "9    2-7 Yrs  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({'Title':job_titles,'Location':job_locations,'Company':company_names,'Experience':experience_required})\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b7c8a6",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for ‚ÄúData Scientist‚Äù Job position in ‚ÄúBangalore‚Äù location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.shine.com/\n",
    "2. Enter ‚ÄúData Analyst‚Äù in ‚ÄúJob title, Skills‚Äù field and enter ‚ÄúBangalore‚Äù in ‚Äúenter the location‚Äù field.\n",
    "3. Then click the searchbutton.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccceb0b9",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95d0ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d9c97ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the shine.com page an automated chrome browser\n",
    "driver.get(\"https://www.shine.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "534428c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ElementNotInteractableException",
     "evalue": "Message: element not interactable\n  (Session info: chrome=124.0.6367.158)\nStacktrace:\n\tGetHandleVerifier [0x00007FF680401522+60802]\n\t(No symbol) [0x00007FF68037AC22]\n\t(No symbol) [0x00007FF680237B13]\n\t(No symbol) [0x00007FF6802809F7]\n\t(No symbol) [0x00007FF68027EB1A]\n\t(No symbol) [0x00007FF6802AAB7A]\n\t(No symbol) [0x00007FF68027A7C6]\n\t(No symbol) [0x00007FF6802AAD90]\n\t(No symbol) [0x00007FF6802CA224]\n\t(No symbol) [0x00007FF6802AA923]\n\t(No symbol) [0x00007FF680278FEC]\n\t(No symbol) [0x00007FF680279C21]\n\tGetHandleVerifier [0x00007FF6807041BD+3217949]\n\tGetHandleVerifier [0x00007FF680746157+3488183]\n\tGetHandleVerifier [0x00007FF68073F0DF+3459391]\n\tGetHandleVerifier [0x00007FF6804BB8E6+823622]\n\t(No symbol) [0x00007FF680385FBF]\n\t(No symbol) [0x00007FF680380EE4]\n\t(No symbol) [0x00007FF680381072]\n\t(No symbol) [0x00007FF6803718C4]\n\tBaseThreadInitThunk [0x00007FF924E47034+20]\n\tRtlUserThreadStart [0x00007FF924FA26A1+33]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mElementNotInteractableException\u001b[0m           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Entering Job title and location as required in the question\u001b[39;00m\n\u001b[0;32m      3\u001b[0m skills\u001b[38;5;241m=\u001b[39mdriver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mCLASS_NAME,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput-label \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m skills\u001b[38;5;241m.\u001b[39msend_keys(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData Analyst\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py:231\u001b[0m, in \u001b[0;36mWebElement.send_keys\u001b[1;34m(self, *value)\u001b[0m\n\u001b[0;32m    228\u001b[0m             remote_files\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upload(file))\n\u001b[0;32m    229\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(remote_files)\n\u001b[1;32m--> 231\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute(\n\u001b[0;32m    232\u001b[0m     Command\u001b[38;5;241m.\u001b[39mSEND_KEYS_TO_ELEMENT, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(keys_to_typing(value)), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: keys_to_typing(value)}\n\u001b[0;32m    233\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py:395\u001b[0m, in \u001b[0;36mWebElement._execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    393\u001b[0m     params \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    394\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent\u001b[38;5;241m.\u001b[39mexecute(command, params)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:347\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n\u001b[0;32m    348\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mElementNotInteractableException\u001b[0m: Message: element not interactable\n  (Session info: chrome=124.0.6367.158)\nStacktrace:\n\tGetHandleVerifier [0x00007FF680401522+60802]\n\t(No symbol) [0x00007FF68037AC22]\n\t(No symbol) [0x00007FF680237B13]\n\t(No symbol) [0x00007FF6802809F7]\n\t(No symbol) [0x00007FF68027EB1A]\n\t(No symbol) [0x00007FF6802AAB7A]\n\t(No symbol) [0x00007FF68027A7C6]\n\t(No symbol) [0x00007FF6802AAD90]\n\t(No symbol) [0x00007FF6802CA224]\n\t(No symbol) [0x00007FF6802AA923]\n\t(No symbol) [0x00007FF680278FEC]\n\t(No symbol) [0x00007FF680279C21]\n\tGetHandleVerifier [0x00007FF6807041BD+3217949]\n\tGetHandleVerifier [0x00007FF680746157+3488183]\n\tGetHandleVerifier [0x00007FF68073F0DF+3459391]\n\tGetHandleVerifier [0x00007FF6804BB8E6+823622]\n\t(No symbol) [0x00007FF680385FBF]\n\t(No symbol) [0x00007FF680380EE4]\n\t(No symbol) [0x00007FF680381072]\n\t(No symbol) [0x00007FF6803718C4]\n\tBaseThreadInitThunk [0x00007FF924E47034+20]\n\tRtlUserThreadStart [0x00007FF924FA26A1+33]\n"
     ]
    }
   ],
   "source": [
    "# Entering Job title and location as required in the question\n",
    "\n",
    "skills=driver.find_element(By.CLASS_NAME,\"input-label \")\n",
    "skills.send_keys('Data Analyst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ac9cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "location=driver.find_element(By.XPATH,\"/html/body/div/div[4]/div/div[2]/div[2]/div/form/div/div[1]/ul/li[2]/div/label\")\n",
    "location.send_keys('Bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc47395c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054e0cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32e5b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d041e80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd19de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ab57a19",
   "metadata": {},
   "source": [
    "Q3: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\n",
    "            As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f900a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d3e273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the Flipkart product review page\n",
    "driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7618b07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store data\n",
    "ratings = []\n",
    "review_summaries = []\n",
    "full_reviews = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "713bb446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more pages available or failed to click the next button.\n",
      "  Rating       Review Summary  \\\n",
      "0      5       Classy product   \n",
      "1      5             Terrific   \n",
      "2      5            Wonderful   \n",
      "3      5    Worth every penny   \n",
      "4      5  Best in the market!   \n",
      "5      5    Terrific purchase   \n",
      "6      5     Perfect product!   \n",
      "7      5     Perfect product!   \n",
      "8      5            Just wow!   \n",
      "9      5            Excellent   \n",
      "\n",
      "                                         Full Review  \n",
      "0  Camera is awesomeBest battery backupA performe...  \n",
      "1                            Very very goodREAD MORE  \n",
      "2                    This is amazing at allREAD MORE  \n",
      "3  Feeling awesome after getting the delivery of ...  \n",
      "4                               Good CameraREAD MORE  \n",
      "5                         Value for money üòçREAD MORE  \n",
      "6                              Photos superREAD MORE  \n",
      "7                                V Good allREAD MORE  \n",
      "8                         Perfect Product!!READ MORE  \n",
      "9                                       NYCREAD MORE  \n"
     ]
    }
   ],
   "source": [
    "# Function to scrap the data from given page\n",
    "\n",
    "def scrape_data():\n",
    "    # Parse the page HTML\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Find all reviews\n",
    "    review_blocks = soup.find_all(\"div\", class_=\"col EPCmJX Ma1fCG\")\n",
    "    \n",
    "    for review_block in review_blocks:\n",
    "        # Extract rating\n",
    "        rating = review_block.find(\"div\", class_=\"XQDdHH Ga3i8K\").text.strip()\n",
    "        ratings.append(rating)\n",
    "        \n",
    "        # Extract review summary\n",
    "        review_summary = review_block.find(\"p\", class_=\"z9E0IG\").text.strip()\n",
    "        review_summaries.append(review_summary)\n",
    "        \n",
    "        # Extract full review\n",
    "        full_review = review_block.find(\"div\", class_=\"ZmyHeo\").div.text.strip()\n",
    "        full_reviews.append(full_review)\n",
    "\n",
    "# Loop to get data from multiple pages until 100 reviews are scraped\n",
    "while len(ratings) < 100:\n",
    "    scrape_data()\n",
    "    \n",
    "    # Click on the \"Next\" button to go to the next page\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH,'//a[@class=\"_9QVEpD\"][last()]')\n",
    "        next_button.click()\n",
    "        \n",
    "        # Wait for the next page to load\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"col EPCmJX Ma1fCG\")))\n",
    "        \n",
    "        # Add a short delay to ensure the page is fully loaded\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        print(\"No more pages available or failed to click the next button.\")\n",
    "        break\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame of the scraped data\n",
    "data = {\n",
    "    'Rating': ratings[:100],\n",
    "    'Review Summary': review_summaries[:100],\n",
    "    'Full Review': full_reviews[:100]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd4eff8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review Summary</th>\n",
       "      <th>Full Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Classy product</td>\n",
       "      <td>Camera is awesomeBest battery backupA performe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific</td>\n",
       "      <td>Very very goodREAD MORE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Wonderful</td>\n",
       "      <td>This is amazing at allREAD MORE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Feeling awesome after getting the delivery of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Good CameraREAD MORE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific purchase</td>\n",
       "      <td>Value for money üòçREAD MORE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Photos superREAD MORE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>V Good allREAD MORE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>Just wow!</td>\n",
       "      <td>Perfect Product!!READ MORE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>NYCREAD MORE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rating       Review Summary  \\\n",
       "0      5       Classy product   \n",
       "1      5             Terrific   \n",
       "2      5            Wonderful   \n",
       "3      5    Worth every penny   \n",
       "4      5  Best in the market!   \n",
       "5      5    Terrific purchase   \n",
       "6      5     Perfect product!   \n",
       "7      5     Perfect product!   \n",
       "8      5            Just wow!   \n",
       "9      5            Excellent   \n",
       "\n",
       "                                         Full Review  \n",
       "0  Camera is awesomeBest battery backupA performe...  \n",
       "1                            Very very goodREAD MORE  \n",
       "2                    This is amazing at allREAD MORE  \n",
       "3  Feeling awesome after getting the delivery of ...  \n",
       "4                               Good CameraREAD MORE  \n",
       "5                         Value for money üòçREAD MORE  \n",
       "6                              Photos superREAD MORE  \n",
       "7                                V Good allREAD MORE  \n",
       "8                         Perfect Product!!READ MORE  \n",
       "9                                       NYCREAD MORE  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5268f9",
   "metadata": {},
   "source": [
    "Q4: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for ‚Äúsneakers‚Äù in the search field.\n",
    "You have to scrape 3 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "As shown in the below image, you have to scrape the above attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42ace142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d2dcd0",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be8b9f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d167b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the Flipkart product review page\n",
    "driver.get(\"https://www.flipkart.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6d4e7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entering Brand name and serch option.\n",
    "Brands=driver.find_element(By.CLASS_NAME,\"Pke_EE\")\n",
    "Brands.send_keys('sneakers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75f5b88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.CLASS_NAME,\"_2iLD__\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e4bbdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand=[]\n",
    "product_description=[]\n",
    "price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d4c1352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Chrome driver\n",
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43fa85a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to Flipkart.com and search for \"sneakers\"\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "search_box = driver.find_element(By.CLASS_NAME,\"Pke_EE\")\n",
    "search_box.send_keys(\"sneakers\")\n",
    "search_button = driver.find_element(By.XPATH,\"//button[@type='submit']\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "684abab2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (302174083.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[48], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    brand = sneaker.find_element(By.XPATH, \"//div[@class=\"syl9yP\"]\").text\u001b[0m\n\u001b[1;37m                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "# Extract the sneaker data\n",
    "sneaker_data = []\n",
    "sneakers = driver.find_elements(By.XPATH,\"/html/body/div/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\")\n",
    "for sneaker in sneakers[:100]:\n",
    "    brand = sneaker.find_element(By.XPATH, \"//div[@class=\"syl9yP\"]\").text\n",
    "    description = sneaker.find_element(By.XPATH, \"//a[@class=\"WKTcLC\"]\").text\n",
    "    price = sneaker.find_element(By.XPATH, \"//div[@class=\"Nx9bqj\"]\").text\n",
    "    sneaker_data.append({\"brand\": brand, \"description\": description, \"price\": price})\n",
    "\n",
    "# Create a pandas DataFrame from the extracted data\n",
    "df = pd.DataFrame(sneaker_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0a1f991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     brand                                        description price\n",
      "0  ROASTER  Stylish Multi Sports Casual Sneaker Shoes Snea...  ‚Çπ317\n"
     ]
    }
   ],
   "source": [
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f8b462",
   "metadata": {},
   "source": [
    "Note\n",
    "i unable to solve question we have face problem need more class for using this application "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
